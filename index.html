<!DOCTYPE html>
<html lang="en">

<head>
    <title>Deep Symbolic Learning: Discovering Symbols and Rules from Perceptions</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">

    <link href="https://fonts.googleapis.com/css?family=Lato:400,400i|PT+Serif:700" rel="stylesheet">
    <link rel="stylesheet" href="dist/css/style.css">
    <link rel="stylesheet" href="dist/css/new_style.css">
    <!-- <link rel="stylesheet" href="css/index.css" /> -->
</head>

<body class="is-boxed has-animations">
    <div class="body-wrap boxed-container">
        <div class="header" id="header">
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-center">
                <h1>Deep Symbolic Learning: Discovering Symbols and Rules from Perceptions </h1>

                <a href="https://dkm.fbk.eu/author/alessandrodaniele/" target="_blank">Alessandro Daniele<sup>1</sup></a>
                <a href="https://www.tommasocampari.com/" target="_blank">Tommaso Campari<sup>1</sup></a>
                <a href="https://countinglogic.github.io/" target="_blank">Sagar Malhotra<sup>1</sup></a>
                <a href="https://dkm.fbk.eu/author/lucianoserafini/" target="_blank">Luciano Serafini<sup>1</sup></a>
            </div>

            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-center">
                <span><sup>1</sup>Fondazione Bruno Kessler (FBK), Trento, Italy. &nbsp; <sup>2</sup>University of Padova, Padova, Italy. &nbsp;
                    <sup>3</sup>University of Trento, Trento, Italy.
                    </span>
            </div>

            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-center">
                [<a href="" target="_blank">GitHub</a>]
                [<a href="https://arxiv.org/abs/2208.11561" target="_blank">Paper</a>]
            </div>

        </div>

        <main>
            <div class="container">
                <div class="row">
                    <div class="has-top-divider">&nbsp;</div>
                    <div class="col-lg-2 col-md-2 col-sm-2 col-xs-2"></div>
                    <div class="col-lg-10 col-md-10 col-sm-10 col-xs-10">
                            <image src="images/teaser.png" class="img-fluid" />
                    </div>
                </div>

                <div class="row">
                    <h2>Abstract</h2>
                    <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-justify">
                        <p>
                            Neuro-Symbolic (NeSy) integration combines symbolic reasoning with Neural Networks (NNs) for tasks requiring perception and reasoning. Most NeSy systems rely on continuous relaxation of logical knowledge, and no discrete decisions are made within the model pipeline. Furthermore, these methods assume that the symbolic rules are given. In this paper, we propose Deep Symbolic Learning (DSL), a NeSy system that learns NeSy-functions, i.e., the composition of a (set of) perception functions which map continuous data to discrete symbols, and a symbolic function over the set of symbols. DSL learns simultaneously the perception and symbolic functions while being trained only on their composition (NeSy-function). The key novelty of DSL is that it can create internal (interpretable) symbolic representations and map them to perception inputs within a differentiable NN learning pipeline. The created symbols are automatically selected to generate symbolic functions that best explain the data. We provide experimental analysis to substantiate the efficacy of DSL in simultaneously learning perception and symbolic functions.
                        </p>
                    </div>

                </div>

                <div class="row">
                    <h2>Approach: DSL</h2>
                    &nbsp;
                    <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-justify">

                        <image class="intro-image" src="images/teaser.png" />

                        <p class="text-justify">
                            In the MultiON task, the agent is given the current goal
                            gi from a set of N goals {g1, g2, ..., gN }. Once the agent has
                            reached gi and generated the Found action successfully, it is
                            given the next goal gi+1. This continues until the agent has
                            found all the goals in the episode. In Modular-MON,
                            we take a modular approach to multi-object navigation by
                            employing the following modules: (1) Object detection (O),
                            (2) Map building (M), (3) Exploration (E) and (4) Navigation (N ). These modules are
                            intuitively
                            weaved
                            together.
                            Modular-MON identifies objects (O) by observing the environment and builds a semantic map
                            (M) by
                            projecting
                            information about category labels of the objects (i.e. semantics) in the field of view. If
                            the agent
                            has
                            not
                            yet discovered
                            the current goal, gi, it will continue to explore (E). Once
                            the current goal has been discovered, Modular-MON plans a
                            path from its current location to the goal, and generates actions to navigate (N) towards
                            the goal.
                            We
                            experiment with
                            different exploration and navigation strategies to systematically investigate their
                            contribution to
                            the
                            agent performance.
                        </p>
                    </div>

                </div>

                <div class="row">
                    <h2>Results</h2>
                    &nbsp;
                    <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-justify">
                        <img class="intro-image" src="images/results-table2.png" />

                        <p>
                            We observe that the PredictedSem agent, which builds a map (M) using predicted
                            semantic labels (O), performs better on cylinder (‘CYL’) objects than natural (‘NAT’)
                            objects.
                            We
                            compare its performance with two oracle agents, OracleMap and OracleSem where ground-truth
                            (‘GT’) is
                            provided for either the
                            mapping or object
                            semantics. As expected, the performance are mostly identical for the two object types for
                            OracleMap
                            and
                            OracleSem, since
                            the placement of the objects are the same for both, with OracleMap outperforming OracleSem.
                            These
                            methods use Uniform
                            Top-down Sampling w/ Fail-Safe (‘Uf’) as the Exploration (E) module and PointNav [52 ]
                            (‘PN’) as
                            the
                            Navigation (N )
                            module.
                        </p>
                        <p>&nbsp;</p>
                        <img class="intro-image" src="images/results-table3.png" />

                        <p>
                            We also investigate how our Modular-MON performs on the
                            Object Goal Navigation (ObjectNav) task in the above table. We observe that the performance
                            deteriorates
                            as we increase target objects, for
                            a fixed step limit (rows 1-3). Our OracleSem performs similarly on the <a
                                href="https://aihabitat.org/challenge/2022/">Habitat ObjectNav 2022</a> and MultiON 2.0
                            1ON
                            val
                            set (rows 5,6) when we set the step limit to 500 steps, following ObjectNav task setting. In
                            ObjectNav,
                            OracleSem
                            performs better (rows 6,7) with Uniform Top-down Sampling w/ Fail-Safe (Uf) than Frontier
                            (F).
                            Moreover,
                            our PredictedSem performs
                            better on MultiON2.0 than on ObjectNav (rows 8,9).
                        </p>

                        <p>
                            Below, we show a rollout of one of our episodes containing one target object.
                        </p>

                        <a href="https://youtu.be/SBm50QlLi3M" target="_blank">
                            <image src="images/rollout.gif" class="img-fluid" />
                        </a>

                    </div>
                </div>

                <div class="row">
                    <h2>Error Modes</h2>
                    &nbsp;
                    <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-justify">
                        <div class="row">
                            <div class="col-lg-2 col-md-2">&nbsp;&nbsp;</div>
                            <div class="col-lg-8 col-md-8 col-sm-12 col-xs-12">
                                <image src="images/failure_cases_mon.png" class="img-fluid img-thumbnail rounded"
                                    alt="failure-mon">
                                </image>
                                <figcaption class="text-center gif-img-caption">
                                    Error modes for Multi-Object Navigation.
                                </figcaption>
                            </div>
                        </div>
                        <div class="row">
                            <p class="text-justify">
                                In the MultiON task, Modular-MON fails due to the agent
                                running out of step limit, or stopping at a location far away
                                from the goal. For those cases where the agent ran out
                                of steps, it either has not yet discovered the goal or has
                                discovered the goal but failed to stop within 1m of it.
                            </p>
                        </div>
                        <div class="row">
                            <div class="col-lg-2 col-md-2">&nbsp;&nbsp;</div>
                            <div class="col-lg-8 col-md-8 col-sm-12 col-xs-12">
                                <image src="images/failure_cases_objnav.png" class="img-fluid img-thumbnail rounded"
                                    alt="failure-objnav">
                                </image>
                                <figcaption class="text-center gif-img-caption">
                                    Error modes for Object Navigation.
                                </figcaption>
                            </div>
                        </div>
                        <div class="row">
                            <p class="text-justify">
                                Modular-MON fails on the ObjectNav task mostly because the agent ran out of steps. Some
                                episodes
                                fail even when the agent is within 1m of the goal bounding
                                box with the goal in sight, indicating that the viewpoints sampled for determining
                                success in ObjectNav are sparse.
                            </p>
                        </div>
                    </div>

                </div>

                <div class="row">
                    <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12">
                        <h2>Citation</h2>
                        <pre xml:space="preserve" style="display: block;">
@article{daniele2022deep,
  title={Deep Symbolic Learning: Discovering Symbols and Rules from Perceptions},
  author={Daniele, Alessandro and Campari, Tommaso and Malhotra, Sagar and Serafini, Luciano},
  journal={arXiv preprint arXiv:2208.11561},
  year={2023}
}
</pre>
                    </div>
                </div>

                <div class="row">
                    <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12">
                        <h2>Acknowledgements</h2>
                        <p class="text-justify">
                            This project was supported by the Alejandro Foundation.
                        </p>
                    </div>
                </div>
            </div>
        </main>
    </div>
    <!-- Bootstrap core JavaScript
        ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>

</body>

</html>
