<!DOCTYPE html>
<html lang="en">

<head>
    <title>Deep Symbolic Learning: Discovering Symbols and Rules from Perceptions</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">

    <link href="https://fonts.googleapis.com/css?family=Lato:400,400i|PT+Serif:700" rel="stylesheet">
    <link rel="stylesheet" href="../dist/css/style.css">
    <link rel="stylesheet" href="../dist/css/new_style.css">
    <!-- <link rel="stylesheet" href="css/index.css" /> -->
    <script type="text/javascript" charset="utf-8" 
src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,
https://vincenttam.github.io/javascripts/MathJaxLocal.js"></script>
</head>

<body class="is-boxed has-animations">
    <div class="body-wrap boxed-container">
        <div class="header" id="header">
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-center">
                <h1>Deep Symbolic Learning: Discovering Symbols and Rules from Perceptions </h1>

                <a href="https://dkm.fbk.eu/author/alessandrodaniele/" target="_blank">Alessandro Daniele<sup>1</sup></a>
                <a href="https://www.tommasocampari.com/" target="_blank">Tommaso Campari<sup>1,2</sup></a>
                <a href="https://countinglogic.github.io/" target="_blank">Sagar Malhotra<sup>1,3</sup></a>
                <a href="https://dkm.fbk.eu/author/lucianoserafini/" target="_blank">Luciano Serafini<sup>1</sup></a>
            </div>

            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-center">
                <span><sup>1</sup>Fondazione Bruno Kessler (FBK), Trento, Italy. &nbsp; <sup>2</sup>University of Padova, Padova, Italy. &nbsp;
                    <sup>3</sup>TU Wien, Wien, Austria.
                    </span>
            </div>
            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-center">
                    Accepted in IJCAI 2023
            </div>

            <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-center">
                [<a href="https://github.com/dkm-fbk/DSL" target="_blank">GitHub</a>]
                [<a href="https://arxiv.org/abs/2208.11561" target="_blank">Paper</a>]
            </div>

        </div>

        <main>
            <div class="container">
                <div class="row">
                    <div class="has-top-divider">&nbsp;</div>
                    <div class="col-lg-2 col-md-2 col-sm-2 col-xs-2"></div>
                    <div class="col-lg-10 col-md-10 col-sm-10 col-xs-10">
                            <image src="../images/teaser.png" class="img-fluid" />
                    </div>
                </div>

                <div class="row"> 
                    <h2>Abstract</h2>
                    <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-justify">
                        <p>
                            Neuro-Symbolic (NeSy) integration combines symbolic reasoning with Neural Networks (NNs) for tasks requiring perception and reasoning. Most NeSy systems rely on continuous relaxation of logical knowledge, and no discrete decisions are made within the model pipeline. Furthermore, these methods assume that the symbolic rules are given. In this paper, we propose Deep Symbolic Learning (DSL), a NeSy system that learns NeSy-functions, i.e., the composition of a (set of) perception functions which map continuous data to discrete symbols, and a symbolic function over the set of symbols. DSL learns simultaneously the perception and symbolic functions while being trained only on their composition (NeSy-function). The key novelty of DSL is that it can create internal (interpretable) symbolic representations and map them to perception inputs within a differentiable NN learning pipeline. The created symbols are automatically selected to generate symbolic functions that best explain the data. We provide experimental analysis to substantiate the efficacy of DSL in simultaneously learning perception and symbolic functions.
                        </p>
                    </div>

                </div>

                <div class="row">
                    <h2>Approach: DSL</h2>
                    &nbsp;
                    <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-justify">

                        <!--image class="intro-image" src="images/teaser.png" /-->

                        <p class="text-justify">
                            The key idea behind DSL is to adapt reinforcement learning policies to the NeSy context: given the NN predictions $\bf t$ with $\sum_i t_i = 1$, we use the greedy policy $\pi({\bf t}) = argmax_i t_i $ to select a single discrete symbol, and the function $\mu({\bf t}) = max_i t_i$ to select the corresponding value in $\bf t$, interpreted as a truth value under a fuzzy logic semantics. For example, in the image above, $\bf t_1$ and $\bf t_2$ are predictions from neural networks $N_1$ and $N_2$ respectively. The policy function $\pi(\bf t_1)$ returns the symbol with the highest truth value in $\bf t_1$, hence returning the symbol $s_8$. Similarly, $\pi(\bf t_2)$ returns the symbol with the highest truth value in $\bf t_1$, i.e.  $s_0$. The functions $\mu(\bf t_1)$ and $\mu(\bf t_2)$ return the highest values in the vectors $\bf t_1$ and $\bf t_2$ respectively, i.e.,  $\mu(\bf t_1) = t_1^{*} = 0.5$ and  $\mu(\bf{t_2}) = t_2^{*} = 0.6$. We can interpret the values $\mu(\bf t_1)$ and $\mu(\bf t_2)$ as the truth values of the propositions $s_8 = x_1$  and $s_0 = x_2$ respectivaly, where $x_1$ and $x_2$ are the unknown labels of the two MNIST images.
                            
                           
                            
                            The symbols $s_8$ and $s_0$ are passed to the symbolic function, which is represented as a lookup table $\bf G$, returning the final output $G[s_0,s_8]$. The confidence in the output ($G[s_0,s_8]$ in this case) is given as the  truth value of the following proposition: 
                            $$(s_0 = x_1) \land (s_8 = x_2)$$ 
                            using the godel semantics of conjunction, i.e., $\bf{t^{*}}$ = $min(\bf{t_1^{*}},\bf{t_2^{*}})$, we have the final confidence value of the prediction. The framework considers the correctness of the final output, producing a label $l=1$ if the prediction is correct and $l=0$ otherwise. Such a label is given as supervision for $\bf t^*$. In the example, the prediction is wrong since the first digit has been classified as $s_8$ instead of $s_3$. At this point, given the wrong final output of the summation, DSL detects the presence of an error. In detail, it creates a label $l$, which is zero if the prediction is wrong and one if it is correct. Such a label is given as supervision for $\bf t^*$. The confidence of the network (i.e., the truth value calculated with the $min$). 
Since the $min$ function admits only one non-zero partial derivative (corresponding to the minimum value), the back-propagation changes the weights of a single network ($N_1$ in the figure above). If the prediction is wrong, the effect of this change is to reduce the truth value of the currently selected symbol ($s_8$), increasing the chances of choosing a different symbol in the next iteration. Finally, \shortname\ can also learn the table $\bf G$ from the data by applying $\pi$ and $\mu$ to a learnable tensor $\bf W$. 
                        </p>
                    </div>

                </div>

                <div class="row">
                    <h2>Results</h2>
                    &nbsp;
                    <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 text-justify">
                        <!--img class="intro-image" src="images/results-table2.png" /-->

                        <p>
                            
                        </p>
                        <p>&nbsp;</p>
                        <!--img class="intro-image" src="images/results-table3.png" /-->

                        <p>
                            
                        </p>

                        <p>

                        </p>

                    

                    </div>
                </div>


                <div class="row">
                    <h2>Citation</h2>
                    <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12">
                        
                        <pre xml:space="preserve" style="display: block;">
@article{daniele2022deep,
  title={Deep Symbolic Learning: Discovering Symbols and Rules from Perceptions},
  author={Daniele, Alessandro and Campari, Tommaso and Malhotra, Sagar and Serafini, Luciano},
  journal={arXiv preprint arXiv:2208.11561},
  year={2023}
}
                        </pre>
                    </div>
                </div>

                <div class="row">
                    <h2>Acknowledgements</h2>
                    <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12">
                        <p class="text-justify">
                            TC and LS acknowledge the support  of the PNRR project FAIR -  Future AI Research (PE00000013),  under the NRRP MUR program funded by the NextGenerationEU.
                        </p>
                    </div>
                </div>
            </div>
        </main>
    </div>
    <!-- Bootstrap core JavaScript
        ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>

</body>

</html>
